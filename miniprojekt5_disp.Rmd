---
title: "Miniprojekt5_disp"
output:
  pdf_document: default
  html_notebook: default
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(CVXR)
library(MASS)
```

## Linear programering 


\begin{itemize}
  \item Kanonisk form:
  \begin{itemize}
    \item max $c_1x_1+\cdots+c_nx_n=c^Tx$
    \item Under betingerlserne.
    \item $\begin{bmatrix} a_{11}x_1+\cdots+a_{1n}x_n \leq b_1 \\ \vdots \\ a_{m1}x_1+\cdots + a_{mn}x_n \leq b_m \end{bmatrix}$ eller $Ax \leq b$.
    \item $x \geq 0$
    \item $x_i$ er varible. $c_i$ er koefficenterne. Som giver kriteriefunktionen
    \item $z=c_1x_1+\cdots + c_nx_n$ som skal maximeres
  \end{itemize}
  \item Standart form:
  \begin{itemize}
    \item max $c_1x_1+\cdots+c_nx_n=c^Tx$
    \item Under betingerlserne.
    \item $\begin{bmatrix} a_{11}x_1+\cdots+a_{1n}x_n = b_1 \\ \vdots \\ a_{m1}x_1+\cdots + a_{mn}x_n = b_m \end{bmatrix}$ eller $Ax = b$.
    \item $x \geq 0$
  \end{itemize}
  \item Et lineart programmerings problem på kanonisk form kan bringes på standard form, ved at indføre nye variable.
  \item $x_{n+1},\ldots,  x_{m+n}$, en for hver bibetingelse. 
  \item max $c_1x_1+\cdots+c_nx_n=\bar{c}^T\bar{x}$
  \item Under betingerlserne.
  \item $\begin{bmatrix} a_{11}x_1+\cdots+a_{1n}x_n+x_{n+1} \leq b_1 \\ \vdots \\ a_{m1}x_1+\cdots + a_{mn}x_n+x_{n+m} \leq b_m \end{bmatrix}$ eller $\bar{A}\bar{x} \leq b$.
  \item $\bar{x} \geq 0$
  \item hvor $\bar{A}=\begin{bmatrix} A, I \end{bmatrix}$
  \item $\bar{c}=\begin{bmatrix} c \\ 0 \end{bmatrix}$
\end{itemize}  

## Simplex

\begin{itemize}
  \item Linear programmeringsproblem på standart form.
  \item $\forall b_i \geq0$
  \item $m \leq n$
  \item Fremgangsmetode: 
  \begin{itemize}
    \item Opstil matrix $\begin{bmatrix} c & 0^T & 0 \\ A & I & b \end{bmatrix}$, for $2n$ variable
    \item Opstil matrix $\begin{bmatrix} c  & 0 \\ A & b \end{bmatrix}$, for $n$ variable
    \item Trin 1: Tjek hvilken indgang i $c$ som har den højeste værdi. Dette er den første søjle vi indrager i vores basis. Kald denne søjle for $j$.
    \item Trin 2: Tjek forholdet mellem $b_i/A_{ij}, \quad \forall i$. Det $i$ som giver det laveste tal, definerer hvilken række der skal pivoteres i. 
    \item Hvis der stadig er positive tal i $c$, gå tilbage til trin 1. Ellers er problemet løst.
    \item Nu er der dannet en basis. Værdierne for $x_1,\ldots, x_n$ kan nu aflæses. Dette gøres ved at tjekke hvilke søjler og indgange der er i pivot i. hvis der er pivot i søjle $j$, indgang $i$. Betyder det, at $x_j$ skal have værdien aflæst i $b_i$. Hvis der ikke er pivot i søjle $j$ er $x_j=0$. 
    \item I indgangen over $b$ kan $-z$ aflæses. 
  \end{itemize}
\end{itemize}

## Lagrange

\begin{itemize}
  \item max $f(x_1, \ldots, x_n)$
  \item Under betingelserne $\begin{bmatrix} g_1(x_1,\ldots, x_n)=c_1 \\ \vdots \\ g_m(x_1,\ldots, x_n)=c_m \end{bmatrix}$ 
  \item ved at danne lagrangeligningen 
  \item $\mathcal{L}(x,y,\ldots, \lambda)  = f(x,y,\ldots) - \lambda(g(x,y,\ldots ) - c$
  \item Sæt $\nabla \mathcal{L} = 0$ og løs ligningssystemet.
\end{itemize}

# Opgaver

## Task 1



```{r}
pivot_matrix <- function(M, pivot){

    rp <- pivot[1]
    cp <- pivot[2]
    z <- (1:nrow(M))[-rp]
    
    M[rp,] <- M[rp,] / M[rp, cp]

    for (i in z){
        k <- M[i, cp]
        M[i, ] <- M[i,] - k* M[rp,]
    }
    M
}

## Profit, der skal maximeres
## P(x) = 5 * x_1 + 4 * x_2 + 6 * x_3

M <- matrix(c(-5, 1, 2,  -4, 1, 1,  -6, 1, 3,  0, 1, 0, 0, 0, 1, 0, 25, 51), nr=3)
M 

## NB: Identitet i søjle 4,5 svarende til de to slack variable.
## (x1=0, x2=0,x3=0, s1=25, s2=51) er brugbar løsning, men P(x)=0

## Størst effekt af at øge x3 (3. søjle).

cp <- 3
M[-1, 6] / M[-1, cp]

## Ligning 2 (3. række) er den kritiske
rp <- 3

## Vi pivoterer om c(rp, cp) = c(3,3)

M2 <- pivot_matrix(M, c(rp,cp))
M2

## Basis i søjle 3,4; foreløbig bud på løsning (x1=0, x2=0,, x3=17 s1=8,
## s2=0); profit er P(x)=17*6 = 102.

## Næste søjle at pivotere efter er x1 (2. søjle).

cp <- 2
M2[-1, 6] / M2[-1, cp]
## Ligning 1 (2. række) er den kritiske
rp <- 2

M3 <- pivot_matrix(M2, c(rp,cp))
M3

## Basis i søjle 2,3;  løsning (x1=0, x2=12, x3 = 13, s1=0,
## s2=0); profit er P(x)= 12 * 4 + 13 * 6 = 126.

## øverste række i første 3 søjler er 0 og derfor er vi i max


```


## Task 2

minimer $A=2 \cdot \pi \cdot r \cdot(h+r)$\newline
mht. $V=\pi \cdot r^2 \cdot h = 1$.

Opstil Lagrangiangen $\mathcal{L}(r,h,\lambda)=2 \cdot \pi \cdot r(h+r)-\lambda(\pi \cdot r^2 \cdot h - 1)$ så løses ligningssystemet givet ved $\nabla \mathcal{L} = 0$.




## Task 3
Betragt følgende situation: Vi har n uafhængige stokastiske variable $y_1,\ldots,y_n$ hvor $y_i\sim N(\mu,\sigma^2v_i^2)$, hvor alle $v_i$'er er kendt, og $\sigma^2$ er ukendt. 

1. Vi ønsker at estimere $\mu$. Lad $\overline{y}=\frac{1}{n}\sum_{i}y_i$. Hvad er $E[\overline{y}]$ og $Var[\overline{y}]$? Kan man mon finde et bedre estimat
for $\mu$ end et simpelt gennemsnit?

\subsubsection{Svar}
Vi anvender lineariteten for at finde $E[\overline{y}]$:
\begin{align*}
E[\overline{y}]=n\frac{1}{n}E[y_i] = \mu
\end{align*}
For at finde variansen indsætter vi blot:
\begin{align*}
Var[\overline{y}] = Var[\frac{1}{n}\sum_{i}y_i]=\frac{1}{n^2}Var[\sum_{i}y_i]=\frac{1}{n^2}Var[y_1+\ldots+y_n]&=\frac{1}{n^2}(Var[y_1]+\ldots+Var[y_n])\\
&=\frac{\sigma^2}{n^2}\sum_iv_i^2
\end{align*}
Man kan sikkert godt finde et bedre estimat, hvis man bruger andet end nulmodellen, men det er svært ud fra de data der er givet.


2. Lad $\tilde{y}=\sum_i p_iy_i$ hvor $p=(p_1,\ldots,p_n)$ er en vektor af kendte tal (som vi skal finde ud af at vÃ¦lge).
Hvilken værdi af $p$ giver at $\tilde{y}$ har mindst mulig varians? Hvilke begrænsninger lægges der på $p_1,\ldots,p_n$ hvis vi Ã¸nsker at $E[\tilde{y}]=\mu$?

\subsection{Svar}
For at finde ud af hvilke værdier $p$ skal have for at minimere variansen kan vi starte med at udregne variansen
\begin{align*}
Var[\tilde{y}]=Var[\sum_i p_iy_i]=Var[p_1y_1+\ldots+p_ny_n]&=p_1^2Var[y_1]+\ldots+p_n^2Var[y_n]\\
&=\sigma^2(p_1^2v_1^2+\ldots+p_n^2v_n^2)\\
&=\sigma^2\sum_ip_i^2v_i^2
\end{align*}
Hvor vi kan se, at vi opnår mindst varians, hvis summen giver 0. Hvis vi vil have at $E[\tilde{y}]=\mu$, kan vi beregne den forventede værdi
\begin{align*}
E[\tilde{y}]=E[\sum_i p_iy_i]=p_1E[y_1]+\ldots+p_nE[y_n] = p_1\mu+\ldots+p_n\mu = \mu\sum_ip_i
\end{align*}
Hvilket viser at denne sum skal give 1 for at dette er tilfældet. 

3. I statistik ønsker man ofte at få et parameterestimat, der er unbiased of har mindst mulig varians. Vi
Ønsker altså at vælge p så $\tilde{y}$ har middelværdi $\mu$ og mindst mulig varians. Løs dette problem ved hjælp
af Lagrange metoden. Løs evt. problemet på en anden måde også.

\subsection{Svar}
For at gøre dette vil vi altså minimere variansen, som vi har fundet til at være 
\begin{align*}
Var[\tilde{y}]=\sigma^2\sum_ip_i^2v_i^2
\end{align*}
I forhold til at middelværdien skal være $\mu$, som vi fandt ud af betød at 
\begin{align*}
\sum_ip_i &= 1\\
1^TP=1
\end{align*}
Vi opstiller derfor lagrangefunktionen:
\begin{align*}
\mathcal{L}(p,\lambda) &= \sigma^2\sum_ip_i^2v_i^2-\lambda(\sum_i p_i-1)\\
\mathcal{L}(p,\lambda) &= p^Tp\sigma^2v^Tv-\lambda(1^Tp-1)
\end{align*}
Vi differentierer nu i forhold til $p$ og sætter det lig 0 for at se hvordan det kommer til at se ud: 
\begin{align*}
\partial/\partial p\mathcal{L}(p,\lambda) = 2\sigma^2p_v_Tv = 0
\end{align*}
Og i forhold til $\lambda$
\begin{align*}
\partial/\partial \lambda\mathcal{L}(p,\lambda) = 1^Tp = 1
\end{align*}

Dette kan sættes op på matrix-vektor form, og man kan derefter løse et ligningssystem.
$\begin{bmatrix} 2 \sigma^2 v_1^2 & 0 & \cdots &-1 \\0 & \ddots & & \vdots \\ \vdots  & & 2\sigma^2v_n^2\\ 1 & 1 & \cdots &0 \end{bmatrix} \begin{bmatrix} P \\ \lambda \end{bmatrix}= \begin{bmatrix} 0 \\ \vdots \\ 1 \end{bmatrix}$.
Hvis dette løses fås $p_i=\frac{\prod_{j \neq i} v_i^2}{\sum_{j}^n v_j^2 \sum_{k>j}v_k^2}$

```{r}
nn = 10
sigma_1 <- 2
v <- runif(nn , 0, 5)
AA <- matrix(data=0, nrow =nn+1, ncol= nn+1)
for (i in 1:nn){
  AA[i,i]= (2*sigma_1^2*v[i]^2)
}
AA[,nn+1] <- -1
AA[nn+1,] <- 1
AA[nn+1,nn+1] <- 0
bb<- rep(0,nn+1)
bb[nn+1] <- 1
sum(solve(AA,bb))-solve(AA,bb)[nn+1]
```



## Task 4


```{r}
data(cars)
glm_cars <- lm(dist  ~ speed, cars)
# sd for beta_1
summary(glm_cars)
# confint for beta_1
confint(glm_cars)
# da beta_1 > 0 allerede, ville bibetingelsen ikke ændre på noget, vist nedeunder

Y <- cars$dist
X <- as.matrix(data.frame(rep(1,length(cars$speed)), cars$speed))
p <- 2
betaHat <- Variable(p)
objective <- Minimize(sum((Y - X %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)
result
plot(cars)
abline(c(-17.579173,3.932414))

# I tilfælde af at det var ment at beta_0 >0  
problem <- Problem(objective, constraints = list(betaHat[1] > 0))
result <- solve(problem)
result
abline(c(3.774416e-08,2.909142e+00))
```
