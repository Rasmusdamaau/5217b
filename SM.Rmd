---
title: "Optimisation: Self study 1 -- Line search"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this self study session we will consider the `cars` dataset (see `?cars`):

```{r}
head(cars)
```

It consists of `r nrow(cars)` cars and was recorded in the 1920s. It has 2 columns: `speed` (speed [mph]) and `dist` (stopping distance [ft]):

```{r}
plot(dist ~ speed, cars)
```

Denote by $(s_i, d_i)$ for $i = 1, 2, \ldots, `r nrow(cars)`$ the observations.

We want to fit a straight line of the form $m(s) = a + b \cdot s$ to the data. We want to determine $a$ and $b$. One way is to minimise the objective function given by
\[
f(a, b) = \frac{1}{n} \sum_{i = 1}^n f_i(a, b),
\]
where
\[
f_i(a, b) = (m(s_i) - d_i)^2 .
\]

## Animation

Below, you are asked to illustrate. You can try both static graphics and with animations (e.g. using the `animation` package: <https://yihui.name/animation/>).

# Exercise 1: Gradient descent

Exercises:

```{r}
#data der skal bruges 
s <- cars$speed
d <- cars$dist


#definerer funktionen f som er summen af residualer 
f <- function(a,b){
  n <- length(s)
  1/n*sum((a + b * s - d)^2)
}

#definerer nu gradienten af f
gradf <- function(a,b){
  2/n*c(sum(a + b * s - d),sum((a + b * s - d) * s))
}



#gradient descent funktion
gradientdescendent <- function(a,b){
  alpha <- 0.001#skridtlængde
  xk <- c(a,b) # punktet defineres som en vektor
  it <- 1 #iterationer
  pk <- -gradf(xk[1],xk[2]) # retning. 
  while (norm(t(gradf(xk[1],xk[2])),"I")>0.0001) {
   xk <- xk + alpha * pk
   pk <- -gradf(xk[1],xk[2])
   it <- it+1
  }#vi kører så længe at gradienten ikke er tæt nok på 0. 
c(f(xk[1],xk[2]),xk,it) #output 
}

gradientdescendent(1,2)
```

Nu prøver vi at lave en gradient descent, men med en variende skridtlængde. Her er det global minimizer. Starter med at definerer funktionen der bestemmer skridtlængden

```{r}
alphanice <- function(a,b,pk){
  -sum(a + b* s - d)/(pk[1] + pk[2] * s)
}

gradientdescendent2 <- function(a,b){
  xk <- c(a,b) # punktet defineres som en vektor
  pk <- -gradf(xk[1],xk[2])
  it <- 1 #iterationer
    while (norm(t(gradf(xk[1],xk[2])),"I")>0.1) {
   alpha <- alphanice(xk[1],xk[2],pk)
   xk <- xk + alpha * pk
   pk <- -gradf(xk[1],xk[2])
   it <- it+1
  }#vi kører så længe at gradienten ikke er tæt nok på 0. 
c(f(xk[1],xk[2]),xk,it) #output 
}

gradientdescendent2(127,-32)
#virker ikke
```



1. What is the gradient of $f$?
2. Implement gradient descent and then use it to find the best straight line.
    * What is meant by *the best* straight line in relation to the objective function above?
    * Discuss different ways to determine the step sizes.
3. Try with different ways to choose step sizes and illustrate it (including plotting the objective function and the iterates, $\{x_k\}_k$). 
    * (Technically, it may be easier to have an algorithm for each way of choosing step size.)
4. Show some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

Account for theoretical properties of the gradient descent.

# Exercise 2: Stochastic gradient descent / incremental gradient descent

In the gradient descent method, all observations are used in each step. If the dataset is really big it may be a problem.

Instead, many smaller steps can be taken (either using one observation at a time or small batches of observations). This is often called stochastic gradient descent or incremental gradient descent and can be described as:

* Choose starting value $x_0$ ($x_0 = (a_0, b_0)$).
* Repeat until convergence:
    + Randomly shuffle the observations in the dataset with a permutation $\sigma$ such that observation $i$ now becomes observation $\sigma(i)$.
    + For each $i = 1, 2, \ldots, n$: take a step using only the $\sigma(i)$'th observation (minimise $f_{\sigma(i)}$ instead of $f$).

Exercises:

1. What is the difference between stochastic gradient descent and gradient descent?
2. How do you think the optimisation path (the path $\left (k, f(x_k) \right )$) looks like for stochastic gradient descent compared to that of the gradient descent?
3. **Optional**: Implement stochastic gradient descent.
4. **Optional**: Illustrate the behaviour of the stochastic gradient descent, including:
    + Different ways to choose step sizes.
    + The total objective function with a discussion of how it differs from a similar plot from the gradient descent method.
    + Some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

# Exercise 3: Be creative!

Open exercise: try to be creative!