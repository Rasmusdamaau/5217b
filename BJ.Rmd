---
title: "BJ"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(numDeriv)
```

In this self study session we will consider the `cars` dataset (see `?cars`):

```{r}
head(cars)
car <- cars
```

It consists of `r nrow(cars)` cars and was recorded in the 1920s. It has 2 columns: `speed` (speed [mph]) and `dist` (stopping distance [ft]):

```{r}
plot(dist ~ speed, cars)
```

Denote by $(s_i, d_i)$ for $i = 1, 2, \ldots, `r nrow(cars)`$ the observations.

We want to fit a straight line of the form $m(s) = a + b \cdot s$ to the data. We want to determine $a$ and $b$. One way is to minimise the objective function given by
\[
f(a, b) = \frac{1}{n} \sum_{i = 1}^n f_i(a, b),
\]
where
\[
f_i(a, b) = (m(s_i) - d_i)^2 .
\]

## Animation

Below, you are asked to illustrate. You can try both static graphics and with animations (e.g. using the `animation` package: <https://yihui.name/animation/>).

# Exercise 1: Gradient descent

Exercises:

1. What is the gradient of $f$?
    * Først defineres de to funktioner, f og f.i
```{r}
s <- cars$speed
d <- cars$dist

ms <- function(ab) ab[1] + ab[2] * cars$speed

f.i <- function(ab) (ms(ab) - cars$dist)^2

f <- function(ab) 1/length(f.i(ab))*sum(f.i(ab))
```

Herefter udregnes gradienten af f.i for at kunne finde retningen $p_k$ i gradient decent.
Funktionen af $f_i$ kan skrives som $f_i(a,b)=(a+bs_i - d_i)^2$, hvorved $\frac{\partial}{\partial a}f_i = 2(a + b s_i - d_i)$ og $\frac{\partial}{\partial b}f_i = 2(a + b s_i - d_i)s_i$. De to funktioner skrives ind som kode, og funktionen $f$ skrives også ind; altså summen af de to $f_i$ funktioner
```{r}
part.a <- function(ab) 1/length(cars$speed) * sum(2*(ab[1] + ab[2]* cars$speed - cars$dist))

part.b <- function(ab) 1/length(cars$speed) * sum(2*(ab[1] + ab[2]* cars$speed - cars$dist)*cars$speed)

grad.ab <- function(ab) c(1/length(cars$speed) * sum(2*(ab[1] + ab[2]* cars$speed - cars$dist)), 1/length(cars$speed) * sum(2*(ab[1] + ab[2]* cars$speed - cars$dist)*cars$speed))

```

2. Implement gradient descent and then use it to find the best straight line.
```{r}
ab_0 <- c(1,1)
alpha <- 0.001
ab <- ab_0
itt <- 0
while (norm(t(grad.ab(ab))) > 0.001){
  itt <- itt + 1
  p_k <- -grad.ab(ab)
  ab <- ab + alpha*p_k
}
```
Forløblig implemtering, dobbelttjekkes med optim-funktionen i R
```{r}
optim(c(1,1),f)
```
Implementeringen giver cirka de korrekte værdier - usikkerheden kan skyldesvirker kun for små $\alpha$, hvorfor det ville være en fordel at implemtere andre måder at vælge alpha på. Med den nuværende skridtlængde benyttes følgende antal itterationer:
```{r}
itt
```
Dette forsøger jeg at forbedre:

    * What is meant by *the best* straight line in relation to the objective function above?
Det er den rette linje, som minimerer residualerne mest.

    * Discuss different ways to determine the step sizes.
Første metode at bestemme 

3. Try with different ways to choose step sizes and illustrate it (including plotting the objective function and the iterates, $\{x_k\}_k$). 
    * (Technically, it may be easier to have an algorithm for each way of choosing step size.)
4. Show some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

Account for theoretical properties of the gradient descent.

# Exercise 2: Stochastic gradient descent / incremental gradient descent

In the gradient descent method, all observations are used in each step. If the dataset is really big it may be a problem.

Instead, many smaller steps can be taken (either using one observation at a time or small batches of observations). This is often called stochastic gradient descent or incremental gradient descent and can be described as:

* Choose starting value $x_0$ ($x_0 = (a_0, b_0)$).
* Repeat until convergence:
    + Randomly shuffle the observations in the dataset with a permutation $\sigma$ such that observation $i$ now becomes observation $\sigma(i)$.
    + For each $i = 1, 2, \ldots, n$: take a step using only the $\sigma(i)$'th observation (minimise $f_{\sigma(i)}$ instead of $f$).

Exercises:

1. What is the difference between stochastic gradient descent and gradient descent?
2. How do you think the optimisation path (the path $\left (k, f(x_k) \right )$) looks like for stochastic gradient descent compared to that of the gradient descent?
3. **Optional**: Implement stochastic gradient descent.
4. **Optional**: Illustrate the behaviour of the stochastic gradient descent, including:
    + Different ways to choose step sizes.
    + The total objective function with a discussion of how it differs from a similar plot from the gradient descent method.
    + Some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

# Exercise 3: Be creative!

Open exercise: try to be creative!
