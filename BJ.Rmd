---
title: "BJ"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(numDeriv)
```

In this self study session we will consider the `cars` dataset (see `?cars`):

```{r}
head(cars)
car <- cars
```

It consists of `r nrow(cars)` cars and was recorded in the 1920s. It has 2 columns: `speed` (speed [mph]) and `dist` (stopping distance [ft]):

```{r}
plot(dist ~ speed, cars)
```

Denote by $(s_i, d_i)$ for $i = 1, 2, \ldots, `r nrow(cars)`$ the observations.

We want to fit a straight line of the form $m(s) = a + b \cdot s$ to the data. We want to determine $a$ and $b$. One way is to minimise the objective function given by
\[
f(a, b) = \frac{1}{n} \sum_{i = 1}^n f_i(a, b),
\]
where
\[
f_i(a, b) = (m(s_i) - d_i)^2 .
\]

## Animation

Below, you are asked to illustrate. You can try both static graphics and with animations (e.g. using the `animation` package: <https://yihui.name/animation/>).

# Exercise 1: Gradient descent

Exercises:

1. What is the gradient of $f$?
    * Først defineres de to funktioner, f og f.i
```{r}
s <- cars$speed
d <- cars$dist

ms <- function(ab) ab[1] + ab[2] * cars$speed

f.i <- function(ab) (ms(ab) - cars$dist)^2

f <- function(ab) 1/length(f.i(ab))*sum(f.i(ab))
```

Herefter udregnes gradienten af f.i for at kunne finde retningen $p_k$ i gradient decent.
Funktionen af $f_i$ kan skrives som $f_i(a,b)=(a+bs_i - d_i)^2$, hvorved $\frac{\partial}{\partial a}f_i = 2(a + b s_i - d_i)$ og $\frac{\partial}{\partial b}f_i = 2(a + b s_i - d_i)s_i$. De to funktioner skrives ind som kode, og funktionen $f$ skrives også ind; altså summen af de to $f_i$ funktioner
```{r}
part.a <- function(ab) 1/length(cars$speed) * sum(2*(ab[1] + ab[2]* cars$speed - cars$dist))

part.b <- function(ab) 1/length(cars$speed) * sum(2*(ab[1] + ab[2]* cars$speed - cars$dist)*cars$speed)

grad.ab <- function(ab) c(1/length(cars$speed) * sum(2*(ab[1] + ab[2]* cars$speed - cars$dist)), 1/length(cars$speed) * sum(2*(ab[1] + ab[2]* cars$speed - cars$dist)*cars$speed))

```

2. Implement gradient descent and then use it to find the best straight line.
```{r}
Fast.alp.optim <- function(x_0,al){
  x_0 <- c(1,1)
  ab <- ab_0
  itt <- 0
  while (norm(t(grad.ab(ab))) > 0.001){
    itt <- itt + 1
    p_k <- -grad.ab(ab)
    ab <- ab + al*p_k
  }
  cat("x*=",ab, ", antal itterationer =",itt)
}
Fast.alp.optim(c(1,1),0.001)
```
Forløblig implemtering, dobbelttjekkes med optim-funktionen i R
```{r}
optim(c(1,1),f)
```
Implementeringen giver cirka de korrekte værdier, dog er der en vis usikkerhed, som kan skyldes tollerencen ved while conditionen, eller skridtlængden, som i forgående script er valgt til en fast værdi. Denne bliver omdefineret senere.

    * What is meant by *the best* straight line in relation to the objective function above?
Det er den rette linje, som minimerer residualerne mest.

    * Discuss different ways to determine the step sizes.
Første metode at bestemme skridtlængden med, er global minimizer. Her skal funktionen $\phi(\alpha)=f(x_k + \alpha p_k)$ optimeres. Dette kan gøres ved at aflede mht. $\alpha$.
Først skrives funktionen $f$, så det er nemmere at overskue, hvorefter det kan afledes.
$$\frac{1}{n}\sum^n_{i=1}f_i(a-\alpha \nabla_a f, b-\alpha \nabla_b f)^2$$
Den afledte udregnes, og funktionen sættes lig $0$, dette giver den optimale værdi for alpha, hvor 
$$\alpha_i=\frac{bs_i+a-d_i}{\nabla_bf \times s_i + \nabla_a f}$$
disse værdier skal altså summes, hvilket giver den optimale værdi for det en givet $(a,b)$ værdi. Der defineres en funktion som kan udregne alpha

%$$\frac{\partial}{\partial \alpha}\phi(\alpha)=$$


Anden metode er ved hjælp af backtracking, eller sufficient decrease condition, her benyttes formlen:
$$f(x_k+\alpha p_k) \leq f(x_k) + c_1 \alpha_k \nabla f_k^T p_k$$
overstående implementers
```{r}
  p_k <- -grad.ab(ab)
  Bck.trk <- function(fun,ab,alpha.bar,c_1,rho){
    alpha <- alpha.bar
    while (abs(fun(ab + alpha*-grad.ab(ab)))>fun(ab)+c_1*alpha*t(grad.ab(ab))%*%-grad.ab(ab)) {
      alpha <- rho*alpha
    }
    alpha
  }
Bck.trk(f,c(1,1),5,0.01,0.5)
```
Med funktionen Bck.trk er det nu muligt at bestemme, for hvilket $\alpha$ funktionen giver en mindre værdi. Det udregnede $\alpha$ er relativt småt, hvilket virker plausibelt, da det før var tilfældet at der kun blev fundet et minimum for små alpha. Backtracking implementeres nu i den originale implementering af gradient decent.
```{r}
optimering.m.backtrack <- function(ab,al){
  ab_0 <- ab
  itt.m.bk <- 0
  while (norm(t(grad.ab(ab))) > 0.001){
    itt.m.bk <- itt.m.bk + 1
    alph <- Bck.trk(f,ab,al,0.001,0.5)
    p_k <- -grad.ab(ab)
    ab <- ab + alph*p_k
  }
  cat("x*=",ab,", antal itterationer =",itt.m.bk)
}
optimering.m.backtrack(c(1,2),1)
```
For at opfylde begge wolfe conditions skal sufficient decrease condition (SDC) benyttes, sammen med curvature condition (CC), hvor følgende ulighed skal være overholdt:
$$-p_k^T\nabla f(x_k + \alpha p_k) \leq -c_2 p_k^T \nabla f_k,$$
hvor $0<c_1<c_2<1$. Dette implementeres også som en funktion;
```{r}
wolf.cc <- function(fun, ab, alpha.bar_2, c_2,rho){
  while (t(-grad.ab(ab))%*%grad.ab(ab+alpha*-grad.ab(ab)) > -c_2*t(-grad.ab(ab))%*%grad.ab(ab)) 
    {
    alpha <- rho*alpha
  }
  alpha
}
wolf.cc(f,c(1,1),1,0.5,0.5)
```
Da både SDC og CC kriterierne nu er implementeret, forsøger jeg at lave en algoritme, hvor begge kriterier er opfyldt. 

Følgende algoritme er fra lektion 1.3, og er refferet til som \textit{Algoritme 3.5}. I denne algoritme benyttes \textit{Algoritme 3.6}, som er den såkaldte zoom algoritme. Denne implementeres først, hvorefter strong Wolfe kan implementeres.
```{r}
g <- grad.ab
zoom <- function(a_lo, a_hi, x_k, c1, c2) {
	f_k <- f(x_k)
	g_k <- g(x_k)
	p_k <- -g_k

	k <- 0
	k_max <- 1000   # Maximum number of iterations.
	done <- FALSE

	while(!done) {
		k <- k + 1
		phi_lo <- f(x_k + a_lo*p_k)

		a_k <- 0.5*(a_lo + a_hi)
		phi_k <- f(x_k + a_k*p_k)
		dphi_k_0 <- g_k%*%p_k
		l_k <- f_k + c1*a_k*dphi_k_0

		if ((phi_k > l_k) || (phi_k >= phi_lo)) {
			a_hi <- a_k
		} else {
			dphi_k <- p_k %*% g(x_k + a_k*p_k)

			if (abs(dphi_k) <= -c2*dphi_k_0) {
			  return(a_k)
			}

			if (dphi_k*(a_hi - a_lo) >= 0) {
				a_hi <- a_lo
			}

			a_lo <- a_k
		}

		done <- (k > k_max)
	}
	
	return(a_k)
}
zoom(0.0001,1,c(1,2),0.001,0.9)

```
Da zoom funktionen nu er implementeret, kan \textit{Algoritme 3.5} nu implementeres

```{r}
alpha <- function(a_0, x_k, c1, c2) {
	a_max <- 4*a_0 # Maximum step length. Can also be given as argument.
	f_k <- f(x_k)
	phi_k <- f_k
	a_1 <- a_0
	a0 <- 0
	a_k <- a_1
	a_k_old <- a0

	k <- 0
	k_max <- 1000   # Maximum number of iterations.
	done <- FALSE
	while(!done) {
		k <- k + 1
		f_k <- f(x_k)
		g_k <- g(x_k)
		p_k <- -g_k

		phi_k_old <- phi_k
		phi_k <- f(x_k + a_k*p_k)
		dphi_k_0 <- g_k%*%p_k
		l_k <- f_k + c1*a_k*dphi_k_0

		if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
		  return(zoom(a_k_old, a_k, x_k, c1, c2))
		}

		dphi_k <- p_k %*% g(x_k + a_k*p_k)

		if (abs(dphi_k) <= -c2*dphi_k_0) {
		  return(a_k)
		}

		if (dphi_k >= 0) {
		  return(zoom(a_k, a_k_old, x_k, c1, c2))
		}

		a_k_old <- a_k
		a_k <- rho*a_k + (1 - rho)*a_max # e.g. rho <- 0.5
		done <- (k > k_max)
	}

	return(a_k)
}
alpha(0.1,c(-15,2),0.001,0.9)
```
Da algoritme 3.5 er implementeret, er det nu muligt at benytte en linesearch, med strong Wolfe conditions, og dervded sikre global konvergens.
```{r}
Strong.Wolfe <- function(ab){
  str.it <- 0
  while (norm(t(grad.ab(ab))) > 0.001){
    str.it <- str.it +1
    p_k <- -grad.ab(ab)
    alp <- alpha(0.1,ab,0.001,0.9)
    ab <- ab + alp*p_k
  }
  cat("x*=",ab,", f(x*)=",f(ab),", antal operationer=",str.it)
}
Strong.Wolfe(c(-20,3))
```

3. Try with different ways to choose step sizes and illustrate it (including plotting the objective function and the iterates, $\{x_k\}_k$). 
    * (Technically, it may be easier to have an algorithm for each way of choosing step size.)


4. Show some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

Account for theoretical properties of the gradient descent.

# Exercise 2: Stochastic gradient descent / incremental gradient descent

In the gradient descent method, all observations are used in each step. If the dataset is really big it may be a problem.

Instead, many smaller steps can be taken (either using one observation at a time or small batches of observations). This is often called stochastic gradient descent or incremental gradient descent and can be described as:

* Choose starting value $x_0$ ($x_0 = (a_0, b_0)$).
* Repeat until convergence:
    + Randomly shuffle the observations in the dataset with a permutation $\sigma$ such that observation $i$ now becomes observation $\sigma(i)$.
    + For each $i = 1, 2, \ldots, n$: take a step using only the $\sigma(i)$'th observation (minimise $f_{\sigma(i)}$ instead of $f$).

Exercises:

1. What is the difference between stochastic gradient descent and gradient descent?
2. How do you think the optimisation path (the path $\left (k, f(x_k) \right )$) looks like for stochastic gradient descent compared to that of the gradient descent?
3. **Optional**: Implement stochastic gradient descent.
4. **Optional**: Illustrate the behaviour of the stochastic gradient descent, including:
    + Different ways to choose step sizes.
    + The total objective function with a discussion of how it differs from a similar plot from the gradient descent method.
    + Some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

# Exercise 3: Be creative!

Open exercise: try to be creative!
